{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility in Tensorflow/PyTorch/JAX\n",
    "\n",
    "If you are reading this, I am probably dead 💀 ...\n",
    "\n",
    "![AI](../images/posts/reproducibility_meme.JPG)\n",
    "\n",
    "FFrom looking for answers on how to make it repeatable and writing them here. So please keep in mind that this post is not finished as there are still unresolved topics about reproducibility that I will try to trace 🕵. If you see any mistakes please email me with an explanation and I will try to fix it. So after this brief introduction shall we begin?\n",
    "\n",
    "Reproducibility is a crucial aspect of scientific research, as it allows others to verify and build upon previous findings. This is especially important in the field of machine learning, where reproducibility allows for more accurate comparisons between different methods and algorithms. In this blog, we will explore the issue of reproducibility in the popular machine learning frameworks [Jax](https://jax.readthedocs.io/en/latest/), [Tensorflow](https://www.tensorflow.org), and [Pytorch](https://pytorch.org). We will discuss the challenges and solutions for achieving reproducibility in these frameworks, as well as the benefits of using reproducible methods in machine learning research.\n",
    "\n",
    "It's worth noting that this blog post will not cover reproducibility in distributed learning environments. I will cover that topic in a separate blog post 📜 focused on distributed learning and reproducibility.\n",
    "\n",
    "In industry, machine learning engineers rely on reproducible results every time they run code. This allows them to improve upon their models and explore different hyperparameter settings. In the research field, reproducibility is especially important for accurately comparing results to those published in papers. I hope you know that if you want to properly compare your results with those of your work, it is necessary to run the same data loaders and datasets as the original researchers, and even better (in my opinion also essential) to run and compare your solution locally on their pipelines. In this blog we will delve into why reproducibility is so important in both industry and research. **Remember that we are talking about results, not performance, as these can be more easily reproduced.**\n",
    "\n",
    "\n",
    "## Randomness in deep learning\n",
    "\n",
    "In deep learning, randomness can come from several sources. For example, when initializing the weights of a neural network, random values are often used to break symmetry and allow for better optimization. Randomness can also come from the stochastic gradient descent (SGD) algorithm, which uses random samples from the training dataset to update the weights. Basically initialization weights, processes that are stochastic dropout for example and as well as data loaders are the jey point you need to focus to have deterministic results. At least on paper but you need to remember that you often work on different environments which also should be reproduced package version, python version etc because maybe in some version they change the behavior of some functions. Lastly, device you will **a lot** about how it is difficult to reproduce on different devices GPU / CPU and across different hardwares also 😵‍💫.\n",
    "\n",
    "Randomness is an inherent aspect of deep learning, and it can come from a number of sources. For instance, when initializing the weights of a neural network, it is common to use random values to break symmetry and facilitate better optimization. Similarly, the stochastic gradient descent (SGD) algorithm relies on random samples from the training dataset to update the weights, introducing another source of randomness.\n",
    "\n",
    "To achieve reproducibility in deep learning, it is important to consider the sources of randomness in your model and how they can be controlled. Basic process that introduce randomness in pipelines are:\n",
    "- initialization weights,\n",
    "- datasets and data loaders,\n",
    "- stochastic operation (such as dropout) that on paper are not deterministic.\n",
    "However, it is also important to consider the impact of the environment on reproducibility. This includes the versions of packages and libraries, the version of Python, and the device (e.g. GPU or CPU) being used. Different hardware and software configurations can lead to variations in the behavior of certain functions, which can affect reproducibility 😵‍💫.\n",
    "\n",
    "### Environment reproducibility\n",
    "\n",
    "To ensure reproducibility in your deep learning projects, it is important to carefully manage your environment. Here are some tips for making your environment reproducible:\n",
    "1. Use a tool like [pyenv](https://realpython.com/intro-to-pyenv/) to manage your Python version. This will ensure that you are using the same version of Python across different environments.\n",
    "2. Create virtual environments for your projects using tools like [venv](https://docs.python.org/3/library/venv.html), [Poetry](https://python-poetry.org), [Conda](https://conda-forge.org), or [pdm](https://github.com/pdm-project/pdm). This will allow you to isolate your project dependencies and avoid conflicts with other packages. To create `venv` just run in shell `python -m venv venv` and voilà. To activate it run `source venv/bin/activate`.\n",
    "3. Keep track of the packages and their versions that you use in your virtual environments. You can use the `pip freeze` command to generate a `requirements.txt` file that lists all of the packages and their versions `pip freeze > requirements.txt`. This will help you recreate the exact same environment later on running `pip install -r requirements.txt`.\n",
    "By keeping track of your environment, you can minimize the impact of differences in package versions and other factors that might lead to variations in your results. This will help you achieve more reproducible results in your deep learning projects.\n",
    "4. Running on the GPU also requires having the same version of CUDA, the cuDNN version, to be able to reproduce the results even on the same GPU.\n",
    "\n",
    "### Random number generators (PRNG)\n",
    "\n",
    "Random number generators (RNGs) are an important component of machine learning frameworks like Pytorch, Tensorflow, and Jax, as well as scientific computing libraries like [Numpy](https://numpy.org). These RNGs use pseudorandom number generators (PRNGs) to produce sequences of numbers that are statistically close to random.\n",
    "\n",
    "PRNGs are based on algorithms that generate a sequence of numbers using a fixed starting point, called the seed 🌱. The generated numbers are determined by the seed and the algorithm used, so they will be the same each time the code is run with the same seed. This can be useful for reproducibility, as it allows you to obtain the same results by setting the same seed. However, it also means that the generated numbers are not truly random and may not be suitable for certain applications. I advise to look into this lecture from [the university of Utah](https://www.math.utah.edu/~alfeld/Random/Random.html) and this [paper](http://www.thesalmons.org/john/random123/papers/random123sc11.pdf).\n",
    "\n",
    "There are various algorithms used by PRNGs, including Monte Carlo techniques and counter-based RNGs. The Python `random` module uses the [Mersenne Twister algorithm](https://www.sciencedirect.com/topics/computer-science/mersenne-twister), which uses a state of `624` integers to generate a sequence of seemingly random numbers. Numpy's `numpy.random` and Pytorch's `torch.rand` packages also use the Mersenne Twister algorithm. Tensorflow's `tensorflow.random` package uses the [Philox algorithm](https://numpy.org/doc/stable/reference/random/bit_generators/philox.html), which is based on counter-based RNGs. Jax's `jax.random` package uses the [ThreeFry](https://bashtage.github.io/randomgen/bit_generators/threefry.html), which is also based on counter-based RNGs algorithm.\n",
    "\n",
    "It is important to keep in mind that these RNGs are not truly random, and they may not be suitable for all applications. If you need truly random numbers, you may need to use a hardware-based RNG, such as a physical device that generates random numbers based on quantum phenomena.\n",
    "\n",
    "\n",
    "### CUDA, cuDNN, cuBLAS,\n",
    "\n",
    "When working with GPU you heard about CUDA for sure. [CUDA](https://pl.wikipedia.org/wiki/CUDA) is a parallel computing platform and programming model developed by NVIDIA for general-purpose computations on graphics processing units (GPUs). It allows developers to use the power of GPUs to accelerate their applications, and it is widely used in machine learning and other fields.\n",
    "\n",
    "[cuDNN](https://blog.roboflow.com/what-is-cudnn/) (CUDA Deep Neural Network library) is a GPU-accelerated library for deep learning developed by NVIDIA. It provides highly optimized implementations of common deep learning operations, such as convolutions and matrix multiplications, which can significantly improve the performance of deep learning models on GPUs.\n",
    "\n",
    "[cuBLAS](https://docs.nvidia.com/cuda/cublas/index.html) (CUDA Basic Linear Algebra Subprograms) is a GPU-accelerated library for linear algebra operations developed by NVIDIA. It provides optimized implementations of common linear algebra operations, such as matrix multiplications and decompositions, which can accelerate the performance of many machine learning algorithms.\n",
    "\n",
    "One of the challenges of reproducibility in machine learning is the use of these GPU-accelerated libraries, which can lead to differences in the behavior of the code on different GPU hardware.\n",
    "\n",
    "To mitigate these issues, it is important to carefully specify the GPU hardware and drivers that you are using, and to ensure that the same hardware and drivers are used when running the code. You can also consider using a GPU cloud service, which can provide a consistent hardware environment for running your code. However, it is worth noting that even with these precautions, there may still be some variability in the results due to differences in the underlying hardware.\n",
    "\n",
    "### XLA\n",
    "\n",
    "XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear algebra developed by Google that is used in the Tensorflow machine learning framework and JAX. It is designed to optimize the performance of linear algebra operations, such as matrix multiplications and decompositions, by generating efficient code for different hardware architectures, including CPUs, GPUs, and Tensor Processing Units (TPUs).\n",
    "\n",
    "One of the benefits of using XLA is that it can improve the performance of Tensorflow models by generating highly optimized code for different hardware architectures. This can be particularly useful for large-scale machine learning tasks, where the performance of the model can have a significant impact on the time required to train and evaluate the model.\n",
    "\n",
    "XLA is also designed to be deterministic, which can help with reproducibility in machine learning. By generating the same code for a given set of inputs, XLA can help ensure that the results of a Tensorflow and Jax model are consistent across different runs and hardware platforms. However, it is worth noting that there may still be some variability in the results due to differences in the underlying hardware and other factors.\n",
    "\n",
    "## Well, after this long introduction, perhaps it's time to code 👩‍💻\n",
    "\n",
    "Firstly we will set some variables. We will only set `CUDA_VISIBLE_DEVICES` to ensure that we are working on one device and `CUBLAS_WORKSPACE_CONFIG`. `CUBLAS_WORKSPACE_CONFIG` maybe needed by Pytorch becouse, bit-wise reproducibility is not guaranteed across toolkit versions because the implementation might differ due to some implementation changes. You will get to choose `:16:8` (may limit overall performance) or `:4096:8` (will increase library footprint in GPU memory by approximately 24MiB). Any of those settings will allow for deterministic behavior even with multiple concurrent streams sharing a single cuBLAS handle.\n",
    "\n",
    "To ensure reproducibility in our deep learning code, we need to set some variables that will control the behavior of our model. Specifically, we will set the `CUDA_VISIBLE_DEVICES` variable to ensure that we are using a specific GPU device, and the `CUBLAS_WORKSPACE_CONFIG` variable to configure cuBLAS, as bit-wise reproducibility is not guaranteed across toolkit versions.\n",
    "\n",
    "The `CUDA_VISIBLE_DEVICES` variable is used to specify which GPU devices are visible to the code. By setting this variable, we can ensure that our code is only using a single GPU device. The `CUBLAS_WORKSPACE_CONFIG` variable can take two values: `:16:8` and `:4096:8`. The `:16:8` setting may limit overall performance, but it allows for deterministic behavior even with multiple concurrent streams sharing a single cuBLAS handle. The `:4096:8` setting will increase the library footprint in GPU memory by approximately 24MiB, but it also allows for deterministic behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set gpu id to test it on one device\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "# You may need to set this variables by PyTorch. It may limit overall performance so be aware of it.\n",
    "%env CUBLAS_WORKSPACE_CONFIG=:16:8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we can import all necessary libraries and set up the environment. But first we will import not machine learning libraries that are also necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we already set some env variables we need to set some additional variables.\n",
    "\n",
    "In addition to the randomness provided by the random module, Python uses a random seed for its hashing algorithm, which affects how objects are stored in sets and dictionaries. This must happen the same way every time in order for GerryChain runs to be repeatable. The way to accomplish this is to set the [environment variable](https://docs.python.org/3.3/using/cmdline.html) `PYTHONHASHSEED`.\n",
    "\n",
    "By utilizing cuDNN, Tensorflow/JAX/PyTorch frameworks can take advantage of the specialized hardware and algorithms provided by NVIDIA GPUs to speed up the training and inference of deep learning models. However the downside of this is lack of reproducibility in some cases so we need to not disable it as it is integrated with CUDA. In Tensorflow now you have a function to make all determinisitc as possible but in previous version you need to do something: In version 1.14, 1.15 and 2.0 you do it by setting `TF_CUDNN_DETERMINISTIC` to `1`, in 2.1-2.8 version you set `TF_CUDNN_USE_FRONTEND` and `TF_DETERMINISTIC_OPS` to `1`. As I am working now on newer version of Tensorflow I am not setting those variables. You can read more about it [here](https://github.com/NVIDIA/framework-determinism/blob/master/doc/tensorflow_status.md).\n",
    "\n",
    "However, for Jax setting `TF_CUDNN_DETERMINISTIC` to `1` allowed me to get reproduciable results so this variable for Jax should be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All frameworks\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "# Jax\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally time for importing our main machine learning modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main imports\n",
    "import random\n",
    "import jax\n",
    "import torch\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# additional imports\n",
    "import torchvision\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax import serialization as flax_serialization\n",
    "from flax.training import train_state, checkpoints\n",
    "import optax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to ensure that our pipeline is reproducible, the final step is to adjust the parameters in our framework.\n",
    "\n",
    "One important consideration when setting these parameters is the use of thread parallelism. Thread parallelism refers to the ability of a computer program to execute multiple threads concurrently. In the context of deep learning, this technique can be used to speed up the training process by allowing different threads to work on different parts of the training data at the same time. This can help to reduce the overall training time and improve the efficiency of the learning algorithm. However this also may influence how reproducible are our results.\n",
    "\n",
    "### Tensorflow\n",
    "\n",
    "To ensure reproducible results in Tensorflow, it's important to properly configure the threading behavior of the runtime. The `tf.config.threading` module provides APIs for configuring the threading behavior of the TensorFlow runtime. The `tf.config.threading.set_inter_op_parallelism_threads` function specifies the number of threads to use for parallel execution of independent operations (also known as \"inter-op\" parallelism), while `tf.config.threading.set_intra_op_parallelism_threads` specifies the number of threads to use for parallel execution of operations within a single op (also known as \"intra-op\" parallelism).\n",
    "\n",
    "In addition to configuring threading behavior, Tensorflow 2.8 introduced a single function for enabling deterministic operations: `tf.config.experimental.enable_op_determinism()`. According to the Tensorflow documentation, when this function is called, \"TensorFlow ops will be deterministic.\"\n",
    "\n",
    "For more information on reproducibility in Tensorflow, you can also check out the resources linked in the original [page]((https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_op_determinism)): the NVIDIA framework determinism [documentation](https://github.com/NVIDIA/framework-determinism/blob/master/doc/tensorflow.md) and the tensorflow-determinism package on [PyPI](https://pypi.org/project/tensorflow-determinism/).\n",
    "\n",
    "### Pytorch\n",
    "In the case of thread parallelism, you can avoid setting everything to just one thread, as Pytorch gives a nice solution to this problem, which you will explore later.\n",
    "\n",
    "There are also a few configs related to cuDNN that you may want to set in order to ensure reproducibility:\n",
    "- `torch.backends.cudnn.deterministic`: A `bool` that, if set to True, causes cuDNN to only use deterministic convolution algorithms.\n",
    "- `torch.backends.cudnn.benchmark`: A `bool` that, if set to True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest.\n",
    "\n",
    "In addition to these configs, Pytorch provides a function called `torch.use_deterministic_algorithms`, which \"configures PyTorch to use deterministic algorithms instead of nondeterministic ones where available, and to throw an error if an operation is known to be nondeterministic (and without a deterministic alternative).\" This can help to ensure reproducibility of your results.\n",
    "\n",
    "For more information on reproducibility in Pytorch, you can refer to the Pytorch [documentation](https://pytorch.org/docs/stable/notes/randomness.html) on randomness and the NVIDIA framework determinism [documentation](https://github.com/NVIDIA/framework-determinism/blob/master/doc/pytorch.md), which includes a section specifically on Pytorch.\n",
    "\n",
    "### JAX\n",
    "\n",
    "Jax is designed to be deterministic, as long as everything is implemented correctly (To my knowledge). In most cases, XLA used by Jax is deterministic on its own. However, there is an issue with cuDNN that can impact the reproducibility of Jax programs. To address this issue, you can set the `TF_CUDNN_DETERMINISTIC` environment variable to `1`, which will make cuDNN deterministic for Jax.\n",
    "\n",
    "It's worth noting that `TF_CUDNN_DETERMINISTIC` is a Tensorflow environment variable, even though Jax is a separate library. This is because Jax is developed by Google, the same company that developed Tensorflow. As a result, there is some overlap between the two libraries, and you may need to use Tensorflow-specific tools to ensure reproducibility in your Jax programs my padawans 🤓."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Tensorflow ################\n",
    "# set the number of threads running on the CPU\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "\n",
    "# set rest of operation to be deterministic\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "################ Pytorch ###################\n",
    "# cudnn settings\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# set rest of operation to be deterministic\n",
    "torch.use_deterministic_algorithms(mode=True, warn_only=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which device we shall be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Devices we will use\")\n",
    "try:\n",
    "    print(torch.cuda.get_device_name(), torch.cuda.device_count())\n",
    "    print(tf.config.list_physical_devices(\"GPU\"))\n",
    "    print(jax.devices())\n",
    "except:\n",
    "    print(\"Well no GPU for you my friend\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to keep in mind when creating a pipeline\n",
    "\n",
    "#### Set seed 🌱\n",
    "\n",
    "Now let's create an array of 2 by 2 random numbers from a Gaussian normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python numbers\n",
    "python_array = [[random.random() for _ in range(2)] for i in range(2)]\n",
    "print(f\"Python output: {python_array}\")\n",
    "# numpy\n",
    "numpy_array = np.random.rand(2, 2)\n",
    "print(f\"Numpy output: {numpy_array}\")\n",
    "# pytorch\n",
    "pytorch_array = torch.rand(2, 2)\n",
    "print(f\"PyTorch output: {pytorch_array}\")\n",
    "# tensorflow\n",
    "tf_array = tf.random.uniform((2, 2))\n",
    "print(f\"Tensorflow output: {tf_array}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to run this code three times and you will se that each time you will get different results.\n",
    "\n",
    "So now let's set seed. **Remember** you need to set seed for all the used packages and better to always set seed for `random` and `numpy` packages as you don't know what package generate random numbers in some frameworks. Also you will now see that we will also test JAX as it is the only package where you need to always provide RNGs to every random number creation so you always have reproducible results (if you will not screw up somehow), well this solution have some annoying minuses.\n",
    "\n",
    "To demonstrate the importance of setting seeds for reproducibility, let's consider a simple example where we generate random numbers using different packages. If you run this code multiple times, you'll notice that the results are different each time. This is because the default behavior of these packages is to generate random numbers based on the current system time, which means that the numbers will be different every time the code is run (I don't know if it is True chatGPT told me that, but results are the same).\n",
    "\n",
    "To ensure reproducibility, it's important to set seeds for all the packages that generate random numbers. This is especially important for the `random` and `numpy` packages, as you may not always know which package is being used to generate random numbers in a particular framework.\n",
    "\n",
    "One exception to this rule is JAX, which requires you to provide a random number generator (RNG) for every random number generation. This means that you must always keep track of this variable holding the RNG. While this solution is effective, it can be a little cumbersome as you have to provide an RNG for every random number generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for the random number generator\n",
    "seed = 0\n",
    "# seting PRNG for random package\n",
    "random.seed(seed)\n",
    "# seting PRNG for numpy\n",
    "np.random.seed(seed)\n",
    "# seting PRNG for pytorch\n",
    "torch.manual_seed(seed)\n",
    "# if you are on cuda you need to set the seed for the cuda PRNG\n",
    "torch.cuda.manual_seed(seed)\n",
    "# seting PRNG for tensorflow\n",
    "tf.random.set_seed(seed)\n",
    "# In tensorflow you can set seed for Python, NumPy and TensorFlow with one function\n",
    "# tf.keras.utils.set_random_seed(seed)\n",
    "# seting PRNG for jax\n",
    "rng = jax.random.PRNGKey(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now it's time to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python numbers\n",
    "python_array = [[random.random() for _ in range(2)] for i in range(2)]\n",
    "print(f\"Python output: {python_array}\")\n",
    "# numpy\n",
    "numpy_array = np.random.rand(2, 2)\n",
    "print(f\"Numpy output: {numpy_array}\")\n",
    "# pytorch\n",
    "pytorch_array = torch.rand(2, 2)\n",
    "print(f\"PyTorch output: {pytorch_array}\")\n",
    "# tensorflow\n",
    "tf_array = tf.random.uniform((2, 2))\n",
    "print(f\"Tensorflow output: {tf_array}\")\n",
    "# JAX\n",
    "jax_array = jax.random.uniform(rng, (2, 2))\n",
    "print(f\"JAX output: {jax_array}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the code with those seeds multiple times you'll notice that the results are <span style=\"color:green\">consistent</span> each time. This demonstrates the effectiveness of setting seeds to ensure reproducibility.\n",
    "\n",
    "It's worth noting that if you re-run the code in a notebook environment, the results may not be consistent because the code is being run in separate cells. To properly test the reproducibility of the code, you should run it multiple times as a single script or restart the notebook each time.\n",
    "\n",
    "In the case of JAX, it's important to keep track of your RNGs and use them correctly in order to ensure reproducibility. For more information on how to use RNGs in JAX and the advantages of this approach, you can refer to the JAX [documentation](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html) on random numbers.\n",
    "\n",
    "### Datasets and Dataloader\n",
    "\n",
    "As we mentioned earlier, the optimizer updates our deep learning model based on incoming data. If we can make this incoming data deterministic, we can achieve reproducible results from our optimizer.\n",
    "\n",
    "To ensure reproducibility for datasets and data loaders in Pytorch and Tensorflow, you can use separate generators for each. This will ensure that the same dataset and data loader flow are generated each time the code is run, which can be useful for testing and debugging, as well as for ensuring that the results of a machine learning experiment are reproducible.\n",
    "\n",
    "It's important to note that JAX does not have a built-in data loader. However, you can still use the techniques described here to ensure reproducibility for your datasets in JAX.\n",
    "\n",
    "Next, let's take a closer look at the typical usage of datasets and data loaders in Pytorch and Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy dataset with pytorch\n",
    "class DummyPytorchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, shape=(4, 2)):\n",
    "        self.data = torch.rand(*shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "# We create two datasets, one for training and one for testing\n",
    "pytorch_dataset_train = DummyPytorchDataset()\n",
    "pytorch_dataset_test = DummyPytorchDataset()\n",
    "\n",
    "# Create dataloaders\n",
    "# Train dataloader should be shuffled and test dataloader should not be shuffled\n",
    "pytorch_dataloader_train = torch.utils.data.DataLoader(\n",
    "    pytorch_dataset_train, batch_size=2, shuffle=True \n",
    ")\n",
    "pytorch_dataloader_test = torch.utils.data.DataLoader(\n",
    "    pytorch_dataset_test, batch_size=2, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy dataset with tensorflow\n",
    "tf_dataset_train = (\n",
    "    tf.data.Dataset.from_tensor_slices(tf.random.normal((4, 2)))\n",
    "    .cache()\n",
    "    .shuffle(4)\n",
    "    .batch(2)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "tf_dataset_test = (\n",
    "    tf.data.Dataset.from_tensor_slices(tf.random.normal((4, 2)))\n",
    "    .batch(2)\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Tensorflow does not have a dataloader, but we can iterate over the dataset or create an iterator\n",
    "# In this case we will use build in function to iterate over the dataset as numpy arrays"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now have a look at the data for the two epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    print(\"Train dataset with shuffle\")\n",
    "    for i, batch in enumerate(pytorch_dataloader_train):\n",
    "        print(f\"Batch id {i} with data {batch}\")\n",
    "\n",
    "    print(\"Test dataset without shuffle\")\n",
    "    for i, batch in enumerate(pytorch_dataloader_test):\n",
    "        print(f\"Batch id {i} with data {batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    print(\"Train dataset with shuffle\")\n",
    "    for i, batch in enumerate(tf_dataset_train.as_numpy_iterator()):\n",
    "        print(f\"Batch id {i} with data {batch}\")\n",
    "\n",
    "    print(\"Test dataset without shuffle\")\n",
    "    for i, batch in enumerate(tf_dataset_test.as_numpy_iterator()):\n",
    "        print(f\"Batch id {i} with data {batch}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the code with previous settings and then restart it, you'll get the same results. However, what if you want to go to a specific checkpoint in your dataset and start from there? If you just load the model weights and optimizers and run the code, you'll get the same data as from the beginning of the first epoch, not the saved data from the checkpoint.\n",
    "\n",
    "To save the state of your data loaders and reproduce the data from a specific point, you can use different approaches depending on the framework you're using. In Tensorflow, I don't know if there is currently way to save the state of a data loader, so I opened this [issue](https://github.com/tensorflow/tensorflow/issues/58925) to find out.\n",
    "\n",
    "In Pytorch, you can seed your workers and pass a `torch.Generator` to the `Dataloader`. This generator will handle shuffling and data pre-processing randomness, and it can be used to save and load the state of the generator. This will allow you to reproduce your data from any point in the training process. You can find more information on this technique in the Pytorch [documentation](https://pytorch.org/docs/stable/data.html#data-loading-randomness).\n",
    "\n",
    "Here's an example of how you can use a torch.Generator to save and load the state of a Pytorch data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take care that each worker has consistent seed\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "# Now we create a generator and pass it to the dataloader\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "pytorch_dataloader_train = torch.utils.data.DataLoader(\n",
    "    pytorch_dataset_train, batch_size=2, shuffle=True, worker_init_fn=seed_worker, generator=g,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's see how we can reproduce the same batches of data when shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_rng = g.get_state()\n",
    "for idx, batch in enumerate(pytorch_dataloader_train):\n",
    "    if idx == 0:\n",
    "        print(batch)\n",
    "\n",
    "# now without loading rng state\n",
    "for idx, batch in enumerate(pytorch_dataloader_train):\n",
    "    if idx == 0:\n",
    "        print(batch)\n",
    "\n",
    "# now we load the rng state\n",
    "g.set_state(pytorch_rng)\n",
    "for idx, batch in enumerate(pytorch_dataloader_train):\n",
    "    if idx == 0:\n",
    "        print(batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned earlier, the state of the random number generator (RNG) is important for reproducing the behavior of a dataset at a specific epoch. Without saving the RNG of the generator, we can't go back to a previous epoch and reproduce the dataset behavior because the shuffling is generated from the RNG, and we need to know its state before shuffling to reproduce it correctly.\n",
    "\n",
    "To illustrate how to set up reproducible training using different frameworks, let's consider the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset as an example. In the following sections, we'll show you examples of reproducible training in Pytorch, Tensorflow, and JAX, and provide a brief summary of each example.\n",
    "\n",
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure once again seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Gnerator for Dataloaders\n",
    "g_train = torch.Generator()\n",
    "g_train.manual_seed(seed)\n",
    "\n",
    "g_test = torch.Generator()\n",
    "g_test.manual_seed(seed)\n",
    "\n",
    "\n",
    "# Parameters for training\n",
    "n_epochs = 4\n",
    "num_workers = 4\n",
    "batch_size_train = 32\n",
    "batch_size_test = 64\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 1000\n",
    "cpu = False # NOTE Change if you want to do on cpu/cuda\n",
    "if cpu:\n",
    "    device = torch.device(\"cpu\") \n",
    "else:\n",
    "    device = torch.device(\"cuda\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST(\n",
    "        \"/mnt/data/alzaig/tmp/\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size_train,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    worker_init_fn=seed_worker, \n",
    "    generator=g_train,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST(\n",
    "        \"/mnt/data/alzaig/tmp/\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size_test,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    worker_init_fn=seed_worker, \n",
    "    generator=g_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    \"\"\"Net class for mnist example\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = torch.nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = torch.nn.Dropout2d()\n",
    "        self.fc1 = torch.nn.Linear(320, 50)\n",
    "        self.fc2 = torch.nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.nn.functional.relu(torch.nn.functional.max_pool2d(self.conv1(x), 2))\n",
    "        x = torch.nn.functional.relu(\n",
    "            torch.nn.functional.max_pool2d(self.conv2_drop(self.conv2(x)), 2)\n",
    "        )\n",
    "        x = x.view(-1, 320)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return torch.nn.functional.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a network and an optimizer\n",
    "network = Net().to(device)\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch: int):\n",
    "    \"\"\"Train the model\n",
    "\n",
    "    Args:\n",
    "        epoch (int): current epoch\n",
    "    \"\"\"\n",
    "    network.train()\n",
    "    start = time.time()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data.to(device))\n",
    "        loss = torch.nn.functional.nll_loss(output, target.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    end_time = time.time() - start            \n",
    "    print(\n",
    "        \"Train Epoch: {} {:.4f}s\\tLoss: {:.6f}\".format(\n",
    "            epoch,\n",
    "            end_time,\n",
    "            loss.item(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def test(epoch: int):\n",
    "    \"\"\"Test the model\n",
    "\n",
    "    Args:\n",
    "        epoch (int): current epoch\n",
    "    \"\"\"\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            target = target.to(device)\n",
    "            output = network(data.to(device))\n",
    "            test_loss += torch.nn.functional.nll_loss(\n",
    "                output, target, reduction='sum'\n",
    "            ).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum().detach().cpu()\n",
    "    end_time = time.time() - start\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(\n",
    "        \"Test set: {:.4f}s Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\".format(\n",
    "            end_time,\n",
    "            test_loss,\n",
    "            correct,\n",
    "            len(test_loader.dataset),\n",
    "            100.0 * correct / len(test_loader.dataset),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": network.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"torch_rng\": torch.get_rng_state(),\n",
    "            'torch_cuda_rng': 0 if cpu else torch.cuda.get_rng_state(),\n",
    "            \"numpy_rng\": np.random.get_state(),\n",
    "            \"python_state\": random.getstate(),\n",
    "            \"generator_dataloader_train\": g_train.get_state(),\n",
    "            \"generator_dataloader_test\": g_test.get_state()\n",
    "        },\n",
    "        f\"/mnt/data/alzaig/tmp/pytorch_model_{epoch}.pth\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We train the model\n",
    "test(0)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure our model is loaded not from cache\n",
    "del network, optimizer\n",
    "\n",
    "# create a new network and optimizer\n",
    "network = Net().to(device)\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "# Load last checkpoint\n",
    "checkpoint = torch.load(f\"/mnt/data/alzaig/tmp/pytorch_model_{n_epochs}.pth\")\n",
    "epoch_last = checkpoint[\"epoch\"]\n",
    "network.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "torch.set_rng_state(checkpoint[\"torch_rng\"])\n",
    "if not cpu:\n",
    "    torch.cuda.set_rng_state(checkpoint['torch_cuda_rng'])\n",
    "g_train.set_state(checkpoint[\"generator_dataloader_train\"])\n",
    "g_test.set_state(checkpoint[\"generator_dataloader_test\"])\n",
    "np.random.set_state(checkpoint[\"numpy_rng\"])\n",
    "random.setstate(checkpoint[\"python_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(epoch_last)\n",
    "for epoch in range(epoch_last + 1, n_epochs + 3):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure our model is loaded not from cache\n",
    "del network, optimizer\n",
    "\n",
    "# create a new network and optimizer\n",
    "network = Net().to(device)\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "# Load second checkpoint\n",
    "checkpoint = torch.load(f\"/mnt/data/alzaig/tmp/pytorch_model_2.pth\")\n",
    "epoch_last = checkpoint[\"epoch\"]\n",
    "network.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "torch.set_rng_state(checkpoint[\"torch_rng\"])\n",
    "if not cpu:\n",
    "    torch.cuda.set_rng_state(checkpoint['torch_cuda_rng'])\n",
    "g_train.set_state(checkpoint[\"generator_dataloader_train\"])\n",
    "g_test.set_state(checkpoint[\"generator_dataloader_test\"])\n",
    "np.random.set_state(checkpoint[\"numpy_rng\"])\n",
    "random.setstate(checkpoint[\"python_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(epoch_last)\n",
    "for epoch in range(epoch_last + 1, n_epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary time:\n",
    "- Can we reproduce the results by restarting the script for CPU? Answer ✅.\n",
    "- Can we reproduce the results by restarting the script for the GPU? Answer ✅\n",
    "- Can we reproduce the results from the checkpoints? Answer ✅\n",
    "- Can we reproduce results on CPU and GPU (same device)? Answer ❌\n",
    "- Can we reproduce results on different CPUs? Answer ❌\n",
    "- Can we reproduce results on different but same GPU? Answer ✅\n",
    "- Can we reproduce the results on different graphics cards? Answer ❌ (but I will check this on a few different cards)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure once again seed\n",
    "tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "# Parameters for training\n",
    "n_epochs = 4\n",
    "batch_size_train = 16\n",
    "batch_size_test = 32\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Tensorflow dataset / Keras dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data(path=\"/mnt/data/alzaig/tmp/mnist.npz\",)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Keras model\n",
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Reshape((28,28,-1)),\n",
    "        tf.keras.layers.Conv2D(filters=10, kernel_size=5),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.Conv2D(filters=20, kernel_size=5),\n",
    "        # sorry I did have small error when I used dropout here so I just removed it\n",
    "        # It will not change enything just for comparison between architectures it will\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(50, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=[\"accuracy\"])\n",
    "# Create checkpoint callback\n",
    "cp = tf.train.Checkpoint(model=model, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test, verbose=2)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=1,\n",
    "        workers=1, #you may need to switch to 0 if you have problems with multiprocessing\n",
    "        batch_size=batch_size_train,\n",
    "    )\n",
    "    cp.write(f\"/mnt/data/alzaig/tmp/tf_models_{epoch}.h5\")\n",
    "    model.evaluate(x_test, y_test, verbose=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load last epoch\n",
    "cp.restore(f\"/mnt/data/alzaig/tmp/tf_models_{n_epochs}.h5\")\n",
    "\n",
    "model.evaluate(x_test, y_test, verbose=2)\n",
    "for epoch in range(n_epochs + 1, n_epochs + 3):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=1,\n",
    "        workers=1, #you may need to switch to 0 if you have problems with multiprocessing\n",
    "        batch_size=batch_size_train,\n",
    "    )\n",
    "    cp.write(f\"/mnt/data/alzaig/tmp/tf_models_{epoch}.h5\")\n",
    "    model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load second epoch\n",
    "cp.restore(f\"/mnt/data/alzaig/tmp/tf_models_2.h5\")\n",
    "\n",
    "model.evaluate(x_test, y_test, verbose=2)\n",
    "for epoch in range(2 + 1, n_epochs + 1):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=1,\n",
    "        workers=1, #you may need to switch to 0 if you have problems with multiprocessing\n",
    "        batch_size=batch_size_train,\n",
    "    )\n",
    "    cp.write(f\"/mnt/data/alzaig/tmp/tf_models_{epoch}.h5\")\n",
    "    model.evaluate(x_test, y_test, verbose=2)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary time:\n",
    "- Can we reproduce the results by restarting the script for CPU? Answer ✅.\n",
    "- Can we reproduce the results by restarting the script for the GPU? Answer ✅\n",
    "- Can we reproduce the results from the checkpoints? Answer ❌ Due to data loaders not reproducible\n",
    "- Can we reproduce results on CPU and GPU (same device)? Answer ❌\n",
    "- Can we reproduce results on different CPUs? Answer ❌\n",
    "- Can we reproduce results on different but same GPU? Answer ✅\n",
    "- Can we reproduce the results on different graphics cards? Answer ❌ (but I will check this on a few different cards)\n",
    "\n",
    "### JAX\n",
    "\n",
    "For our example of reproducible training in JAX, we'll be using the [Flax](https://flax.readthedocs.io/en/latest/) library to build our deep neural network (DNN). Flax is an overlay library built on top of JAX that provides a more intuitive interface for building DNNs. It does not add any additional functionality to JAX, but it makes it easier to build DNNs using JAX, which was not designed specifically for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure once again seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "rng = jax.random.PRNGKey(seed)\n",
    "\n",
    "# Gnerator for Dataloaders\n",
    "g_train = torch.Generator()\n",
    "g_train.manual_seed(seed)\n",
    "\n",
    "g_test = torch.Generator()\n",
    "g_test.manual_seed(seed)\n",
    "\n",
    "# Parameters for training\n",
    "n_epochs = 4\n",
    "num_workers = 4\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "  \"\"\"A simple CNN model.\"\"\"\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, training=False):\n",
    "    x = nn.Conv(features=10, kernel_size=(5, 5))(x)\n",
    "    x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Conv(features=20, kernel_size=(5, 5))(x)\n",
    "    x = nn.Dropout(0.2)(x, deterministic=not training)\n",
    "    x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = nn.relu(x)\n",
    "    x = x.reshape((x.shape[0], -1))  # flatten\n",
    "    x = nn.Dense(features=50)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dropout(0.2)(x, deterministic=not training)\n",
    "    x = nn.Dense(features=10)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits, labels):\n",
    "    \"\"\"Compute the cross-entropy loss given logits and labels.\n",
    "\n",
    "    Args:\n",
    "        logits: output of the model\n",
    "        labels: labels of the data\n",
    "    \"\"\"\n",
    "    labels_onehot = jax.nn.one_hot(labels, num_classes=10)\n",
    "    return optax.softmax_cross_entropy(logits=logits, labels=labels_onehot).mean()\n",
    "\n",
    "\n",
    "def compute_metrics_jax(logits, labels):\n",
    "    \"\"\"Compute metrics for the model.\n",
    "\n",
    "    Args:\n",
    "        logits: output of the model\n",
    "        labels: labels of the data\n",
    "    \"\"\"\n",
    "    loss = cross_entropy_loss(logits=logits, labels=labels)\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "    return {'loss': loss, 'accuracy': accuracy}\n",
    "\n",
    "\n",
    "def create_train_state(rng, learning_rate, momentum):\n",
    "    \"\"\"Creates initial `TrainState` with `sgd`.\"\"\"\n",
    "    cnn = CNN()\n",
    "    rng, rng_dropout = jax.random.split(rng)\n",
    "    params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params'] # initialize parameters by passing a template image\n",
    "    tx = optax.sgd(learning_rate, momentum)\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=cnn.apply, params=params, tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, image, label, dropout_rng):\n",
    "  \"\"\"Train for a single step. Also jit-compiled for speed.\n",
    "  \n",
    "  Args:\n",
    "    params: parameters of the model\n",
    "    image: input image\n",
    "    label: label of the image\n",
    "    dropout_rng: rng for dropout\n",
    "  \"\"\"\n",
    "  def loss_fn(params):\n",
    "    logits = CNN().apply({'params': params}, x=image, training=True, rngs={'dropout': dropout_rng})\n",
    "    loss = cross_entropy_loss(logits=logits, labels=label)\n",
    "    return loss, logits\n",
    "  grad_fn = jax.grad(loss_fn, has_aux=True)\n",
    "  grads, logits = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  metrics = compute_metrics_jax(logits=logits, labels=label)\n",
    "  return state, metrics\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(params, image, label):\n",
    "  \"\"\"Evaluate for a single step. Also jit-compiled for speed.\n",
    "  \n",
    "  Args:\n",
    "    params: parameters of the model\n",
    "    image: input image\n",
    "    label: label of the image\n",
    "  \"\"\"\n",
    "  logits = CNN().apply({'params': params}, image)\n",
    "  return compute_metrics_jax(logits=logits, labels=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(state, train_ds, epoch, rng):\n",
    "  \"\"\"Train for a single epoch.\n",
    "  \n",
    "  Args:\n",
    "    params: model parameters\n",
    "    test_ds: test dataloader\n",
    "    epoch: current epoch\n",
    "  \"\"\"\n",
    "  batch_metrics = []\n",
    "  start = time.time()\n",
    "  for batch_idx, (data, target) in enumerate(train_ds):\n",
    "    rng, rng_dropout = jax.random.split(rng)\n",
    "    state, metrics = train_step(state, data, target, rng_dropout)\n",
    "    batch_metrics.append(metrics)\n",
    "\n",
    "  # compute mean of metrics across each batch in epoch.\n",
    "  batch_metrics_np = jax.device_get(batch_metrics)\n",
    "  epoch_metrics_np = {\n",
    "      k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
    "      for k in batch_metrics_np[0]} # jnp.mean does not work on lists\n",
    "\n",
    "  end_time = time.time() - start\n",
    "  print('train epoch: %d, time %.4f,, loss: %.4f, accuracy: %.2f' % (\n",
    "      epoch, end_time, epoch_metrics_np['loss'], epoch_metrics_np['accuracy'] * 100))\n",
    "  return state, rng\n",
    "\n",
    "\n",
    "def eval_model(params, test_ds):\n",
    "  \"\"\"Evaluate the model.\n",
    "  \n",
    "  Args:\n",
    "    params: model parameters\n",
    "    test_ds: test dataloader\n",
    "  \"\"\"\n",
    "  batch_metrics = []\n",
    "  start = time.time()\n",
    "  for batch_idx, (data, target) in enumerate(test_ds):\n",
    "    metrics = eval_step(params, data, target)\n",
    "    batch_metrics.append(metrics)\n",
    "    \n",
    "  # compute mean of metrics across each batch in epoch.\n",
    "  batch_metrics_np = jax.device_get(batch_metrics)\n",
    "  metrics = {\n",
    "      k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
    "      for k in batch_metrics_np[0]} # jnp.mean does not work on lists\n",
    "  end_time = time.time() - start\n",
    "  summary = jax.tree_util.tree_map(lambda x: x.item(), metrics) # map the function over all leaves in metrics\n",
    "  return summary['loss'], summary['accuracy'], end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations applied on each image => bring them into a numpy array\n",
    "def image_to_numpy(img):\n",
    "    img = np.array(img, dtype=np.float32)\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    return img\n",
    "\n",
    "# We need to stack the batch elements\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST(\n",
    "        \"/mnt/data/alzaig/tmp/\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                image_to_numpy\n",
    "            ]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size_train,\n",
    "    shuffle=True,\n",
    "    collate_fn=numpy_collate,\n",
    "    num_workers=num_workers,\n",
    "    worker_init_fn=seed_worker, \n",
    "    generator=g_train,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST(\n",
    "        \"/mnt/data/alzaig/tmp/\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                image_to_numpy\n",
    "            ]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size_test,\n",
    "    shuffle=False,\n",
    "    collate_fn=numpy_collate,\n",
    "    num_workers=num_workers,\n",
    "    worker_init_fn=seed_worker, \n",
    "    generator=g_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng, init_rng = jax.random.split(rng)\n",
    "state = create_train_state(init_rng, learning_rate, momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy, end_time = eval_model(state.params, test_loader)\n",
    "print(' test epoch: %d, time: %.4f, loss: %.2f, accuracy: %.2f' % (\n",
    "      0, end_time, test_loss, test_accuracy * 100))\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  state, rng = train_epoch(state, train_loader, epoch, rng)\n",
    "  # Evaluate on the test set after each training epoch\n",
    "  test_loss, test_accuracy, end_time = eval_model(state.params, test_loader)\n",
    "  print(' test epoch: %d, time: %.4f, loss: %.2f, accuracy: %.2f' % (\n",
    "      epoch, end_time, test_loss, test_accuracy * 100))\n",
    "  # Save the model parameters\n",
    "  numpy_rng = np.random.get_state()\n",
    "  python_state = random.getstate()\n",
    "  torch_state = torch.get_rng_state() # well only this is required but for the sake of completeness I am adding all\n",
    "  checkpoints.save_checkpoint(\n",
    "      ckpt_dir=\"/mnt/data/alzaig/tmp/jax_checkpoints_{}\".format(epoch),\n",
    "      target={\"state\": state,\n",
    "              \"epoch\": epoch,\n",
    "              \"rng\": rng,\n",
    "              \"pytorch_rng\": torch_state.numpy(),\n",
    "              \"numpy_rng\": numpy_rng,\n",
    "              \"python_state\": python_state,\n",
    "              \"generator_dataloader_train\": g_train.get_state().numpy(),\n",
    "              \"generator_dataloader_test\": g_test.get_state().numpy(),\n",
    "            },\n",
    "            step=epoch,\n",
    "            overwrite=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model parameters\n",
    "load_dict = checkpoints.restore_checkpoint(ckpt_dir=f\"/mnt/data/alzaig/tmp/jax_checkpoints_{n_epochs}\", target=None)\n",
    "rng = load_dict[\"rng\"]\n",
    "torch.set_rng_state(torch.tensor(load_dict[\"pytorch_rng\"]))\n",
    "python_state = list(load_dict[\"python_state\"].values())\n",
    "python_state[1] = tuple(python_state[1].values())\n",
    "g_train.set_state(torch.tensor(load_dict[\"generator_dataloader_train\"]))\n",
    "g_test.set_state(torch.tensor(load_dict[\"generator_dataloader_test\"]))\n",
    "np.random.set_state(tuple(load_dict[\"numpy_rng\"].values()))\n",
    "random.setstate(python_state)\n",
    "epoch_last = load_dict[\"epoch\"]\n",
    "state = flax_serialization.from_state_dict(state, load_dict['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy, end_time = eval_model(state.params, test_loader)\n",
    "print(' test epoch: %d, time: %.4f, loss: %.2f, accuracy: %.2f' % (\n",
    "      epoch_last, end_time, test_loss, test_accuracy * 100))\n",
    "\n",
    "for epoch in range(epoch_last+1, n_epochs + 3):\n",
    "  state, rng = train_epoch(state, train_loader, epoch, rng)\n",
    "  # Evaluate on the test set after each training epoch\n",
    "  test_loss, test_accuracy, end_time = eval_model(state.params, test_loader)\n",
    "  print(' test epoch: %d, time: %.4f, loss: %.2f, accuracy: %.2f' % (\n",
    "      epoch, end_time, test_loss, test_accuracy * 100))\n",
    "  # Save the model parameters\n",
    "  numpy_rng = np.random.get_state()\n",
    "  python_state = random.getstate()\n",
    "  torch_state = torch.get_rng_state() # well only this is required but for the sake of completeness I am adding all\n",
    "  checkpoints.save_checkpoint(\n",
    "      ckpt_dir=\"/mnt/data/alzaig/tmp/jax_checkpoints_{}\".format(epoch),\n",
    "      target={\"state\": state,\n",
    "              \"epoch\": epoch,\n",
    "              \"rng\": rng,\n",
    "              \"pytorch_rng\": torch_state.numpy(),\n",
    "              \"numpy_rng\": numpy_rng,\n",
    "              \"python_state\": python_state,\n",
    "              \"generator_dataloader_train\": g_train.get_state().numpy(),\n",
    "              \"generator_dataloader_test\": g_test.get_state().numpy(),\n",
    "            },\n",
    "            step=epoch,\n",
    "            overwrite=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the second model parameters\n",
    "load_dict = checkpoints.restore_checkpoint(ckpt_dir=f\"/mnt/data/alzaig/tmp/jax_checkpoints_2\", target=None)\n",
    "rng = load_dict[\"rng\"]\n",
    "torch.set_rng_state(torch.tensor(load_dict[\"pytorch_rng\"]))\n",
    "python_state = list(load_dict[\"python_state\"].values())\n",
    "python_state[1] = tuple(python_state[1].values())\n",
    "g_train.set_state(torch.tensor(load_dict[\"generator_dataloader_train\"]))\n",
    "g_test.set_state(torch.tensor(load_dict[\"generator_dataloader_test\"]))\n",
    "np.random.set_state(tuple(load_dict[\"numpy_rng\"].values()))\n",
    "random.setstate(python_state)\n",
    "epoch_last = load_dict[\"epoch\"]\n",
    "state = flax_serialization.from_state_dict(state, load_dict['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy, end_time = eval_model(state.params, test_loader)\n",
    "print(' test epoch: %d, time: %.4f, loss: %.2f, accuracy: %.2f' % (\n",
    "      epoch_last, end_time, test_loss, test_accuracy * 100))\n",
    "\n",
    "for epoch in range(epoch_last+1, n_epochs + 1):\n",
    "  state, rng = train_epoch(state, train_loader, epoch, rng)\n",
    "  # Evaluate on the test set after each training epoch\n",
    "  test_loss, test_accuracy, end_time = eval_model(state.params, test_loader)\n",
    "  print(' test epoch: %d, time: %.4f, loss: %.2f, accuracy: %.2f' % (\n",
    "      epoch, end_time, test_loss, test_accuracy * 100))\n",
    "  # Save the model parameters\n",
    "  numpy_rng = np.random.get_state()\n",
    "  python_state = random.getstate()\n",
    "  torch_state = torch.get_rng_state() # well only this is required but for the sake of completeness I am adding all\n",
    "  checkpoints.save_checkpoint(\n",
    "      ckpt_dir=\"/mnt/data/alzaig/tmp/jax_checkpoints_{}\".format(epoch),\n",
    "      target={\"state\": state,\n",
    "              \"epoch\": epoch,\n",
    "              \"rng\": rng,\n",
    "              \"pytorch_rng\": torch_state.numpy(),\n",
    "              \"numpy_rng\": numpy_rng,\n",
    "              \"python_state\": python_state,\n",
    "              \"generator_dataloader_train\": g_train.get_state().numpy(),\n",
    "              \"generator_dataloader_test\": g_test.get_state().numpy(),\n",
    "            },\n",
    "            step=epoch,\n",
    "            overwrite=True,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary time:\n",
    "- Can we reproduce the results by restarting the script for CPU? Answer ✅.\n",
    "- Can we reproduce the results by restarting the script for the GPU? Answer ✅\n",
    "- Can we reproduce the results from the checkpoints? Answer ✅\n",
    "- Can we reproduce results on CPU and GPU (same device)? Answer ❌\n",
    "- Can we reproduce results on different CPUs? Answer ❌\n",
    "- Can we reproduce results on different but same GPU? Answer ✅\n",
    "- Can we reproduce the results on different graphics cards? Answer ❌ (but I will check this on a few different cards)\n",
    "\n",
    "### (Optional) Time to clean up before the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to check if there is nothing valuable in the directory before deleting it\n",
    "# I even recommend you to do that checking everything in the directory before deleting it\n",
    "!rm -rf /tmp/*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize\n",
    "\n",
    "In this blog, we explored the issue of reproducibility in the popular machine learning frameworks Jax, Tensorflow, and Pytorch. We discussed the challenges and solutions for achieving reproducibility in these frameworks, as well as the benefits of using reproducible methods in machine learning research. We also noted that this blog post did not cover reproducibility in distributed learning environments, which will be the focus of a separate blog post. \n",
    "\n",
    "One of the key takeaways from this blog is the importance of setting seeds in order to achieve reproducible results. This is a minimal requirement in the industry, but there are other approaches that can also be useful, particularly in the context of research comparisons. However, it's important to keep in mind that some of these approaches may come at the cost of reduced performance, so they may not be ideal in all situations. That being said, reproducibility can be particularly useful for debugging purposes, even if it may not be the most efficient option in terms of performance.\n",
    "\n",
    "In this blog, we attempted to summarize the current state of reproducibility in these frameworks, including the ability to reproduce results by restarting the script on both CPU and GPU, using checkpoints, and on different hardware configurations. Please note that this blog is still a work in progress, as we are continuing to research and test these issues on different environments and hardware. If you notice any mistakes or have any additional insights to share, please don't hesitate to contact me, my lord.\n",
    "\n",
    "## Acknowledgment\n",
    "\n",
    "I would like to express my gratitude to all the writers and developers who have worked on implementing and documenting reproducible solutions for the various frameworks. In particular, I would like to thank the developers at [OpenAI]((https://openai.com)) for creating ChatGPT, which has made writing this blog much easier.\n",
    "\n",
    "![chatGPT](../images/posts/chatGPT_meme.png)\n",
    "\n",
    "## References:\n",
    "\n",
    "- Python documentation on using the command line (PYTHONHASHSEED): https://docs.python.org/3.3/using/cmdline.html\n",
    "### Pytorch\n",
    "- Pytorch documentation on randomness: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "- Pytorch determinism documentation: https://github.com/NVIDIA/framework-determinism/blob/master/doc/pytorch.md\n",
    "### Tensorflow\n",
    "- Tensorflow issue on reproducibility: https://github.com/tensorflow/tensorflow/issues/53771\n",
    "- Tensorflow documentation on op determinism: https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_op_determinism\n",
    "- Tensorflow determinism tutorial: https://suneeta-mall.github.io/2019/12/22/Reproducible-ml-tensorflow.html\n",
    "- Tensorflow determinism package: https://pypi.org/project/tensorflow-determinism/\n",
    "- Tensorflow determinism documentation: https://github.com/NVIDIA/framework-determinism/blob/master/doc/tensorflow.md\n",
    "- Tensorflow determinism status documentation: https://github.com/NVIDIA/framework-determinism/blob/master/doc/tensorflow_status.md\n",
    "- Tensorflow documentation on threading: https://www.tensorflow.org/api_docs/python/tf/config/threading\n",
    "### Jax\n",
    "- Jax documentation on random numbers: https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html\n",
    "- Flax issue on reproducibility: https://github.com/google/flax/issues/33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 (main, Dec 17 2022, 19:41:24) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dfe15c34b75b152c10117a31565f3711e3ab0d4d3a9a9b2b113488b6d8acfc01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
